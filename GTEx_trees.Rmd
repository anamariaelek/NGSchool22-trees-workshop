---
title: "Tree Based Methods Workshop"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_depth: 3
---

## Goal

In this workshop we are going to use tree-based methods to construct Gene Regulatory Networks (GRNs).

## Motivation

The expression of each gene is regulated by a set of transcription factors (TFs). If a level of TF expression is changed (e.g. a TF is knocked-down), the expression level of it's downstream target genes will be afffected. We can therfore build a model that predicts expression of a particular gene based on the expression of it's potential regulators. Then, by grouping TFs and their targets based on this coordinated co-expression, we can obtain simple Gene Regulatory Networks (GRNs).

Tree based methods are particularly suitable for this problem because:
 * their interpretability allows direct selection of important TFs,
 * they make essentially no assumptions about the nature of the relationships between TF and gene expression (which can be non-linear), 
 * they can potentially capture high-order conditional dependencies between expression patterns.
 
## Setup

```{r setup,  message=FALSE, warning=FALSE}
library(data.table)
library(stringr)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(ipred)
library(ranger)
```

## Data

GTEx portal contains gene expression data for various human tissues, you can access the data [here](https://www.gtexportal.org/home/datasets#filesetFilesDiv131).  
We will work with 578 lung samples.

```{r}
url <- "https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/gene_reads/gene_reads_2017-06-05_v8_lung.gct.gz"
dt <- fread(url)
dim(dt)
dt
```

There are 56,200 genes (variables) in our data. We will include additional column to specify if the gene is a TF or not, which will be used later to select model features. You can download a list of 1639 human TFs from the curated database [here](http://humantfs.ccbr.utoronto.ca/download.php).

```{r}
url <- "http://humantfs.ccbr.utoronto.ca/download/v_1.01/TFs_Ensembl_v_1.01.txt"
tfs <- readLines(url)
head(tfs)
length(tfs)

# strip transcript ID from GTEx data
dt[,Name := str_remove(Name, "\\.\\d+$")]
dt[, .N, Name][, .N, N] # this is to check that we still have all unique entries 

# add TF info
dt[,TF := ifelse(Name %in% tfs, TRUE, FALSE)]
setcolorder(dt, c("id", "Name", "Description", "TF"))

# most of TFs are present in our GTEx data.
dt[, .N, TF]

# we will use more informative gene names instead of ensembl IDs
tfs <- dt[TF == TRUE, Description]
```

Let's do some exploratory data analysis.

```{r}
tpm <- as.data.frame(t(dt[,-c(1:4)]))
colnames(tpm) <- dt$Description
quants <- t(apply(tpm, 1, quantile))
head(quants)
```

We can remove genes that are not expressed, or are very lowly expressed in most of the samples.

```{r warning=FALSE, message=FALSE}
# remove genes not expressed in any sample
not_expressed <- which(colSums(tpm) == 0)
length(not_expressed)
tpm <- tpm[, -not_expressed]
quants <- t(apply(tpm, 1, quantile))
head(quants)

# TPM distribution for TFs vs non-TFs
dtm <- melt.data.table(
  dt[Description %in% colnames(tpm)], 
  id.vars = colnames(dt)[1:4], 
  variable.name = "Sample", 
  value.name = "TPM"
)
ggplot(dtm, aes(TPM, fill=TF)) + 
  geom_histogram(bins=100) + 
  scale_x_log10() +
  scale_fill_viridis_d() +
  geom_vline(xintercept = 100) +
  facet_grid(TF~., scales = "free_y", labeller = label_both) +
  theme(legend.position = "none") 

# remove lowly expressed genes (important for TFs, i.e. variables)
low_expressed <- which(colSums(tpm) < 100)
length(low_expressed)
tpm <- tpm[, -low_expressed]

# how many TFs (i.e. variables) we have left?
tfs <- tfs[tfs %in% colnames(tpm)]
length(tfs)
nrow(tpm)
```

**Note**: A more sophisticated feature selection methods are available, but they will be covered in tomorrow's lectures.  

```{r pca, include=FALSE, eval=FALSE}
pca_out <- prcomp(tpm) # this can take a few minutes for many samples

require(factoextra)
fviz_eig(pca_out)
fviz_pca_ind(pca_out, repel = TRUE)
ggsave("plots/pca.pdf")
```

Split the data into train and test samples.

```{r}
set.seed(1950)
tpm_train_id <- sample(1:nrow(tpm), size = 0.7*nrow(tpm))
tpm_train <- tpm[tpm_train_id, ]; nrow(tpm_train)
tpm_test  <- tpm[-tpm_train_id, ]; nrow(tpm_test)
```

First we will fit different regression trees to predict individual genes expression. We will start with an example from SFTP gene family, which encodes lung surfactant proteins (SFTPA1, SFTPA2, SFTPC, SFTPB, and SFTPD), known to be specifically expressed in lungs. Same approach can then be applied to all other genes.

```{r}
# outcome variable
gene <- "SFTPA1"

# predictor variables
tfs <- setdiff(tfs, gene)
```

## Regression Tree Model

Use the `tree()` function to fit decision tree model.

```{r}
require(rpart)

set.seed(1950)

# build tree
tpm_tree <- rpart(
  formula = as.formula(paste(gene, "~ .", collapse = " ")),
  data = tpm_train[,c(gene,tfs)],
  method = "anova" # for regression
)

# plot tree
require(rpart.plot)
rpart.plot(tpm_tree)
```

The tree is showing the percentage of data that fall to each node, and the average TPM for corresponding branch

Notice that there are only 8 splits in the tree. This is because `rpart` automatically performs a 10-fold cross validation for a range of cost complexity $\alpha$ values (`cp` parameter passed to `rpart.control`) to prune the tree. We can look at the reduction in cross-validation error with increased number of splits.

```{r}
plotcp(tpm_tree)
```

**Note**: This CV error is equivalent to predicted residual error sum of squares (PRESS); it is not equivalent to the root mean squared error (RMSE) which we will calculate later.

```{r}
# get stats for different cp values
tpm_tree$cptable
```

In addition to the cost complexity, other `rpart`` parameters we can tune are:  

* `minsplit`, the minimum number of data points required to attempt a split( before it is forced to create a terminal node (default is 20)  
* `maxdepth`, the maximum number of internal nodes between the root node and the terminal nodes (default is 30).  

In order to automatically tune these parameters, we create a grid search and iterate through it. 

```{r message=FALSE}
hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 15, 1)
)
hyper_grid$i <- 1:nrow(hyper_grid)

# loop over grid search
set.seed(1950)
tpm_trees <- lapply(hyper_grid$i, function(i) {
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]

  # train a model 
  message(i, " / ", nrow(hyper_grid))
  rpart(
    formula = as.formula(paste(gene, "~ .", collapse = " ")),
    data = tpm_train[, c(gene,tfs)],
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
    )
})
```

After building all the models, we select the one with lowest CV error.

```{r}
# get minimum error and associated cp for every tree 
hyper_grid_vals <- lapply(tpm_trees, function(x) {
  min <- which.min(x$cptable[, "xerror"])
  c(
    cp = x$cptable[min, "CP"] ,
    xerror = x$cptable[min, "xerror"] 
  )
})
hyper_grid_vals <- do.call('rbind', hyper_grid_vals)

# add results to grid
hyper_grid <- cbind(hyper_grid[, c("i","minsplit","maxdepth")], hyper_grid_vals)

# order by CV error
hyper_grid <- hyper_grid[order(hyper_grid$xerror, decreasing = FALSE), ]
head(hyper_grid)

ggplot(hyper_grid, aes(minsplit, maxdepth, fill = xerror)) + 
  geom_tile() + coord_fixed() + scale_fill_viridis_c(direction = -1)
```

Notice that the error for top model is lower than for the first tree built with the default parameters.  

Finally, use the best model for prediction with test data.

```{r include=FALSE}
# build the model using best parameters from grid search
tpm_final_tree <- rpart(
  formula = as.formula(paste(gene, "~ .", collapse = " ")),
  data = tpm_train[, c(gene,tfs)],
  method  = "anova",
  control = list(
    minsplit = hyper_grid$minsplit[1],
    maxdepth = hyper_grid$maxdepth[1],
    cp = hyper_grid$cp[1]
  )
)
```

```{r}
tpm_best_tree <- tpm_trees[[ hyper_grid[1,"i"] ]]
pred <- predict(tpm_best_tree, newdata = tpm_test[, tfs])
RMSE(pred = pred, obs = tpm_test[, gene])
```

## Bagging

We can train a regression tree with bagging in a similar way, using `ipred::bagging` function. The major parameter for bagging is the number of trees to average (`nbagg`, default is 25). Increasing this value results in lower error, but after the initial drop, it soon becomes stable.   

```{r message=FALSE, eval=FALSE, echo=FALSE}
require(ipred)

set.seed(1950)

# get OOB errors for models with range of bagged trees
# this is lengthy, use sparser sequence if you want to run it
ntree <- seq(102, 200, 2)

rmse <- sapply(ntree, function(i) {
  bt <- bagging(
    formula = as.formula(paste(gene, "~ .", collapse = " ")),
    data = tpm_train[,c(gene,tfs)],
    coob = TRUE,
    nbagg = i
  )
  bt$err
})
rmse_df <- data.frame(ntree, rmse)
fwrite(rmse_df, file = sprintf("data/rmse.bagging.%s.tsv",gene))
```

```{r}
# load precalculated values
rmse_df <- fread(sprintf("data/rmse.bagging.%s.tsv",gene))
ggplot(rmse_df, aes(ntree, rmse)) + geom_line() + geom_vline(xintercept = 25, col = "red")

# fit model using these parameters
bagging_tree <- bagging(
  formula = as.formula(paste(gene, "~ .", collapse = " ")),
  data = tpm_train[,c(gene,tfs)],
  coob = TRUE,
  nbagg = rmse_df[which.min(rmse_df$ntree),]$ntree
)
```

We can use `caret::train()` which can fit a variety of different models (see `names(getModelInfo())`). It also performs cross-validation and can output variable importance.  

```{r}
require(caret)

# set up a 10-fold cross validation
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagging_tree <- caret::train(
  x = tpm_train[,tfs],
  y = tpm_train[,gene],
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
)
```

**Note**: We could in theory use the same formula syntax as with `pred`, however, `ipred` has a problem with formula parsing of some of the column names, so we specify `x` and `y` values directly)

```{r}
# assess results
bagging_tree

# plot most important variables
plot(varImp(bagging_tree), 30)  
```

Use bagging model for prediction with test data.

```{r}
pred <- predict(bagging_tree, newdata = tpm_test[, tfs])
RMSE(pred = pred, obs = tpm_test[, gene])
```

Notice that this is a slight improvement over simple regression tree.

## Random Forest

There are several parameters to tune:  

* `mtry`, number of randomly selected variables, rule of the thumb is `sqrt(number of variables)`
* `sample.fraction`, faction of observations to sample (default is 1)
* `min.node.size`, minimal node size (default is 5 for regression)

We will again construct the hyperparameter grid to iterate over (note that this time we will not retain the trees, but only the associated errors, for the sake of size).

```{r eval=FALSE}
require(ranger)

# hyperparameter grid search
hyper_grid <- expand.grid(
  ntree = 200,
  mtry = seq(20, 60, by = 2),
  sample.fraction = seq(0.5, 1, 0.1),
  min.node.size = seq(4, 12, by = 2)
)
# total number of combinations
nrow(hyper_grid)

# loop
hyper_grid_vals <- sapply(1:nrow(hyper_grid), function(i) {
  rf_tree <- ranger(
    x = tpm_train[,tfs],
    y = tpm_train[,gene],
    num.trees = hyper_grid[i,"ntree"],
    mtry = hyper_grid[i,"mtry"],
    min.node.size = hyper_grid[i,"min.node.size"],
    sample.fraction = hyper_grid[i,"sample.fraction"],
    importance = "impurity"
  )
  sqrt(rf_tree$prediction.error)
})

# add to grid
hyper_grid$rmse <- hyper_grid_vals

# save
fwrite(hyper_grid, file = sprintf("data/rmse.rf.%s.tsv",gene))
```

Build a model with best parameters obtained from grid search.

```{r}
# load grid search results
hyper_grid <- fread(sprintf("data/rmse.rf.%s.tsv",gene))

# plot
ggplot(hyper_grid, aes(sample.fraction, mtry, fill = rmse)) + 
  geom_tile() + scale_fill_viridis_c(direction = -1)

# build model with best params
i <- which.min(hyper_grid$rmse)
rf_tree <- ranger(
  x = tpm_train[,tfs],
  y = tpm_train[,gene],
  num.trees = hyper_grid[i,]$ntree,
  mtry = hyper_grid[i,]$mtry,
  min.node.size = hyper_grid[i,]$min.node.size,
  sample.fraction = hyper_grid[i,]$sample.fraction,
  importance = "impurity"
)
```

Use Random Forest model for prediction with test data.

```{r}
pred <- predict(rf_tree, tpm_test[, tfs])
RMSE(pred = pred$predictions, obs = tpm_test[, gene])
```

